{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07171723",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(180deg,rgb(92, 0, 128) 0%,rgb(46, 0, 153) 75%, rgb(65, 0, 230) 100%); color: white; font-family: 'Raleway', sans-serif; padding: 10px 20px; border-radius: 10px; text-align: center; font-weight:500;\">\n",
    "Normalización\n",
    "</h1>\n",
    "<br>\n",
    "\n",
    "**PRESENTAN** Armando Islas, Daniela Flores, Oscar Berrueco, Natalia Anaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import emoji\n",
    "import logging\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar recursos de NLTK\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# Crear directorios de salida si no existen\n",
    "OUTPUT_DIR = \"../outputs/normalized/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd62b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto_basico(texto):\n",
    "    \n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"  \n",
    "    # Eliminar URLs\n",
    "    texto = re.sub(r'https?://\\S+|www\\.\\S+', '', texto)\n",
    "\n",
    "    # Eliminar menciones (@usuario)\n",
    "    texto = re.sub(r'@\\w+', '', texto)\n",
    "    \n",
    "    # Eliminar emojis\n",
    "    texto = emoji.replace_emoji(texto, replace='')\n",
    "    \n",
    "    # Eliminar símbolos irrelevantes pero conservar signos importantes para el clickbait\n",
    "    texto = re.sub(r'[^\\w\\s\\?\\!\\.\\,\\:\\;\\…]', '', texto)\n",
    "    \n",
    "    # Eliminar espacios múltiples\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    \n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52ab81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar_texto(texto):\n",
    "    \n",
    "    if not isinstance(texto, str):\n",
    "        return []\n",
    "    \n",
    "    return word_tokenize(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c667ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_stopwords(tokens):\n",
    "    \n",
    "    return [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e11b2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematizar_texto(texto):\n",
    "    \n",
    "    if not isinstance(texto, str):\n",
    "        return []\n",
    "    \n",
    "    doc = nlp(texto)\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4f27732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_stemming(tokens):\n",
    "\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c197cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_token_stop(texto):\n",
    "    \n",
    "    tokens = tokenizar_texto(texto)\n",
    "    tokens_sin_stop = eliminar_stopwords(tokens)\n",
    "    return ' '.join(tokens_sin_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49bf08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_token_stop_lemma(texto):\n",
    "    \n",
    "    lemas = lematizar_texto(texto)\n",
    "    lemas_sin_stop = [lema for lema in lemas if lema.lower() not in stop_words]\n",
    "    return ' '.join(lemas_sin_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "497a8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_all(texto):\n",
    "\n",
    "    texto_limpio = limpiar_texto_basico(texto)\n",
    "    lemas = lematizar_texto(texto_limpio)\n",
    "    lemas_sin_stop = [lema for lema in lemas if lema.lower() not in stop_words]\n",
    "    stems = aplicar_stemming(lemas_sin_stop)\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b85df205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_dataset(input_path):\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_path)\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "    \n",
    "    resultados = {}\n",
    "    \n",
    "    # Verificar si la columna existe\n",
    "    if 'Teaser Text' not in df.columns:\n",
    "        return {}\n",
    "    \n",
    "    # Aplicar técnica 1: tokenización + eliminación de stopwords\n",
    "    df_token_stop = df.copy()\n",
    "    df_token_stop['Teaser Text'] = df_token_stop['Teaser Text'].apply(normalizar_token_stop)\n",
    "    resultados['token_stop'] = df_token_stop\n",
    "    \n",
    "    # Aplicar técnica 2: tokenización + stopwords + lematización\n",
    "    df_token_stop_lemma = df.copy()\n",
    "    df_token_stop_lemma['Teaser Text'] = df_token_stop_lemma['Teaser Text'].apply(normalizar_token_stop_lemma)\n",
    "    resultados['token_stop_lemma'] = df_token_stop_lemma\n",
    "    \n",
    "    # Aplicar técnica 3: todas las técnicas\n",
    "    df_all = df.copy()\n",
    "    df_all['Teaser Text'] = df_all['Teaser Text'].apply(normalizar_all)\n",
    "    resultados['all'] = df_all\n",
    "    \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97f0d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_resultados(resultados):\n",
    "    for tecnica, df in resultados.items():\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"normalized_{tecnica}.csv\")\n",
    "        try:\n",
    "            df.to_csv(output_path, index=False)\n",
    "        except:\n",
    "            print('No se pudo guardar el archivo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bdc95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/TA1C_dataset_detection_train.csv\"\n",
    "\n",
    "# Verificar si el archivo existe\n",
    "if not os.path.exists(input_path):\n",
    "    sys.exit(1)\n",
    "\n",
    "resultados = procesar_dataset(input_path)\n",
    "\n",
    "if resultados:\n",
    "    guardar_resultados(resultados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Entorno ESCOM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
